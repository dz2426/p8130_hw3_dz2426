---
title: "hw3_dz2426"
author: "Duzhi Zhao"
date: "10/7/2019"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(leaflet)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12, 
  fig.height = 10,
  out.width = "100%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1
# Description of "Instacart" dataset
```{r include = FALSE}
# Import data set "instacart"
library(p8105.datasets)
data("instacart")

skimr::skim(instacart) # Summary statistics
```
Comments:

This limited "Instacart" dataset contains 1,384,617 observations of 131,209 unique users, and 15 columns of variables. Some key variables are "aisle"  "department", "product_name", "reordered", and "order_dow".

"aisle" contains 134 groups, such as yogurt, fresh vegetables, and fresh fruits. "product_name" includes 39123 products, such as Bulgarian Yogurt, Organic Celery Hearts, and Bag of Organic Bananas. "department" includes 21 departments, such as dairy eggs, produce, canned goods. "reordered" shows whether the item has been ordered by this user in the past. "order_dow" indicates the day of the week on which the order is placed

# Section 1.1
```{r echo = FALSE}
instacart_data_q1 = instacart %>% 
  janitor::clean_names() %>% 
  count(aisle) %>% #count the number of items ordered from each aisle
  arrange(desc(n)) #sort in descending order
```
Comments:

There are `r range(pull(instacart, aisle_id))[2]` aisles in total and the aisle "fresh vegetables" has the most items ordered from.

# Section 1.2
```{r echo = FALSE}
items_order_df = instacart %>% 
  janitor::clean_names() %>% 
  count(aisle) %>% #count the number of items in each aisle
  filter(n > 10000) %>% #filter items ordered > 10000
  rename(count = n)

items_order_plot = 
  items_order_df %>% 
  mutate(aisle = as.factor(aisle)) %>% #factor aisle into different levels
  ggplot(aes(x = aisle, y = count)) +
  geom_col() + #show bar graph
  coord_flip() + #xy axis flip 
  labs(
    title = "Figure 1: Number of items ordered in each aisle (>10000)"
  ) +
  ylim(0, 170000) 
items_order_plot
```

# Section 1.3
```{r echo = FALSE}
pop_items_df = 
  instacart %>% 
  select(aisle_id, aisle, product_name) %>% #select the data we want
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% #filter out these three aisle groups
  count(aisle, product_name) %>% #count the number of each product under each aisle 
  group_by(aisle) %>% #order by group
  top_n(n = 3) %>% #select the top 3 popular products
  mutate(
    rank = dense_rank(desc(n)) #rank each product by each group
  ) %>% 
  arrange(aisle, rank) %>% 
  rename("ordered_times" = 3) %>% 
  select(-rank)

```

Table 1: The three most popular products in each of the aisles
```{r echo = FALSE}
knitr::kable(pop_items_df)
```

```{r echo = FALSE}
pink_coffee_df = instacart %>% 
  select(order_hour_of_day, order_dow, product_name) %>% #select the data we are interested in
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% #filter out these two products
  group_by(order_dow, product_name) %>% 
  arrange(order_dow, order_hour_of_day) %>% 
  mutate(
    mean_order_hour = round(mean(order_hour_of_day) , digits = 0)
  )

pink_coffee_table = tibble(
  mean_order_hour = c(pull(pink_coffee_df, mean_order_hour)),
  order_day = c(pull(pink_coffee_df, order_dow)),
  product_name = c(pull(pink_coffee_df, product_name)) #create variables we want
) %>% 
  distinct() %>% #remove dupplicated rows
  mutate(
    order_day = recode(order_day, 
                       "0" = "Mon",
                       "1" = "Tue",
                       "2" = "Wed",
                       "3" = "Thu",
                       "4" = "Fri",
                       "5" = "Sat",
                       "6" = "Sun")
  ) %>% #transform integer to name of each day
  pivot_wider(
    names_from = product_name,
    values_from = mean_order_hour
  ) %>% #2x7 table
  janitor::clean_names() %>% 
  rename(pink_lady_apples_order_hr = 2) %>% 
  rename(coffee_ice_cream_order_hr = 3) #clean up names

```

# Section 1.4
Table 2: mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
```{r echo = FALSE}
knitr::kable(pink_coffee_table)
```

## Problem 2
```{r echo = FALSE}
#Import and clean data set
data("brfss_smart2010")

brfss2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr,  #rename variables
         county = locationdesc,
         health_class = class,
         response_id = respid) %>% 
  filter(topic == "Overall Health", 
         response != "Very good") %>% #filter the topic and filter out the response
  mutate(
    response = as.factor(response) #change type to factor
  ) %>% 
  arrange(desc(response)) #arrange from poor to excellent in descending order
```

# Section 2.1
```{r echo = FALSE}
locations2002_df = 
  brfss2010 %>% 
  filter(year == 2002) %>% 
  count(state) %>% 
  mutate(
    n_of_locations = n/4 #After observing dataset, I notice there are 4 responses for each location. Thus, the number of each distinct locations equals the total number of observations divded by 4
  ) %>% 
  filter(n_of_locations > 6) 

locations2010_df = 
  brfss2010 %>% 
  filter(year == 2010) %>% 
  count(state) %>% 
  mutate(
    n_of_locations = n/4 
  ) %>% 
  filter(n_of_locations > 6) 
```
Comments:

In 2002, `r c(pull(locations2002_df, state))` were observed at 7 or more locations. In 2010, `r c(pull(locations2010_df, state))` were observed at 7 or more locations.

# Section 2.2
```{r echo = FALSE}
# Construct dataset limited to "Excellent" response
excellent_resp = 
  brfss2010 %>% 
  group_by(year, state) %>% 
  select(year, state, response, data_value) %>% 
  filter(response == "Excellent")  %>% 
  drop_na() %>% #remove NA 
  mutate(
    mean_data_val = round(mean(data_value), digits = 2) #mean of data_value grouped by year and state
  ) %>% 
  select(-data_value,
         -response) %>% 
  distinct() #remove duplicated rows
```

```{r echo = FALSE}
excellent_resp %>% 
  ggplot(aes(x = year, y = mean_data_val, color = state)) +
  geom_line() +
  xlab("Year") +
  ylab("Average value") +
  ggtitle("Figure 2a: Average value within each state from 2002 to 2010")
```
 
```{r echo = FALSE}
excellent_resp %>% 
  ggplot(aes(x = year, y = mean_data_val, group = state)) +
  geom_point(size = 1) +
  geom_line() +
  facet_wrap(~state, nrow = 4) + 
  xlab("Year") +
  ylab("Average value") +
  ggtitle("Figure 2b: Figure 2a shown with separate plots based on states") +
  theme(axis.text.x = element_text(size = 5),
        axis.text.y = element_text(size = 8),
        axis.title = element_text(size = 15),
        strip.text = element_text(size = 10),
        title = element_text(size = 13),
        panel.spacing.x = unit(1, "lines")
  )
```

# Section 2.3 

```{r echo = FALSE}
NY_response = 
  brfss2010 %>% 
  select(year, state, county, data_value, response) %>% 
  filter(year == 2006 | year == 2010,
         state %in% c("NY")) #Select data

NY_response %>% 
  ggplot(aes(x = response, y = data_value, fill = county)) + 
  geom_col(position = "dodge") +
  facet_grid(~year) + 
  ylab("data value") +
  ggtitle("Figure 3: Distribution of data value for responses (“Poor” to “Excellent”) in NY") + 
  theme(title = element_text(size = 12),
        axis.title = element_text(size = 10))
  
```

## Problem 3
# Section 3.1
```{r echo = FALSE}
# Import dataset
accel_data = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
      day = ordered(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")), #change day into factor and reorder in a classical Mon-Sun way
      if_weekday = ifelse(
      day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") == TRUE, "weekday", "weekend") #determine if it is a weekday
  ) %>% 
  subset(select = c(1, 2, 3, 1444, 4:1443)) #reorder columns
  
```
Comments: 

The tidied "accel_data" dataset contains `r dim(accel_data)[1]` observations and `r dim(accel_data)[2]` variables. Variable "week" includes number 1-5, indicating the number of weeks this experiment was at. Variable "day" refers to the day the accelerometer was monitoring on. Variable "if_weekday" determines if the day is a weekday or weeekend. The rest of the "activity" columns represent the activity counts for each minute of a 24-hour day starting at midnight. A total of 1440 minutes of activity data was included.

Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

# Section 3.2
```{r echo = FALSE}
accel_total = accel_data %>% 
  mutate(
    total_activity = select(., activity_1:activity_1440) %>% 
      rowSums(na.rm = TRUE) %>% #select activity columns and sum all columns on each row
      round(digits = 0) #round to the nearest integer
  ) %>% 
  select(week, day, total_activity) %>%  #remove all minutely activity columns
  arrange(day) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  )
```
Comments:

Table 3: Total activity in minutes for each day over 5 weeks
```{r echo = FALSE}
knitr::kable(accel_total)
```

# Section 3.3
