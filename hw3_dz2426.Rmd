---
title: "hw3_dz2426"
author: "Duzhi Zhao"
date: "10/7/2019"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(leaflet)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12, 
  fig.height = 10,
  out.width = "100%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1
# Description of "Instacart" dataset
```{r include = FALSE}
# Import data set "instacart"
library(p8105.datasets)
data("instacart")

skimr::skim(instacart) # Summary statistics
```
Comments:

write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations

This limited "Instacart" dataset contains 1,384,617 observations of 131,209 unique users, and 15 columns of variables. Some key variables are "aisle"  "department", "product_name", "reordered" showing whether the item has been ordered by this user in the past, and "order_dow" indicating the day of the week on which the order is placed.

"Aisle" has 134 groups, such as yogurt, fresh vegetables, and fresh fruits. "Product_name" includes 39123 products, such as Bulgarian Yogurt, Organic Celery Hearts, and Bag of Organic Bananas. "Department" includes 21 departments, such as dairy eggs, produce, canned goods.

# Section 1.1
```{r echo = FALSE}
instacart_data_q1 = instacart %>% 
  janitor::clean_names() %>% 
  count(aisle) %>% #count the number of items ordered from each aisle
  arrange(desc(n)) #sort in descending order
```
Comments:

There are `r range(pull(instacart, aisle_id))[2]` aisles in total and the aisle "fresh vegetables" has the most items ordered from.

# Section 1.2
```{r echo = FALSE}
items_order_df = instacart %>% 
  janitor::clean_names() %>% 
  count(aisle) %>% #count the number of items in each aisle
  filter(n > 10000) %>% #filter items ordered > 10000
  rename(count = n)

items_order_plot = 
  items_order_df %>% 
  mutate(aisle = as.factor(aisle)) %>% #factor aisle into different levels
  ggplot(aes(x = aisle, y = count)) +
  geom_col() + #show bar graph
  coord_flip() + #xy axis flip 
  labs(
    title = "Figure 1: Number of items ordered in each aisle (>10000)"
  ) +
  ylim(0, 170000) 
items_order_plot
```

# Section 1.3
```{r echo = FALSE}
pop_items_df = 
  instacart %>% 
  select(aisle_id, aisle, product_name) %>% #select the data we want
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% #filter out these three aisle groups
  count(aisle, product_name) %>% #count the number of each product under each aisle 
  group_by(aisle) %>% #order by group
  top_n(n = 3) %>% #select the top 3 popular products
  mutate(
    rank = dense_rank(desc(n)) #rank each product by each group
  ) %>% 
  arrange(aisle, rank) %>% 
  rename("ordered_times" = 3) %>% 
  select(-rank)

```

Table 1: The three most popular products in each of the aisles
```{r echo = FALSE}
knitr::kable(pop_items_df)
```

```{r echo = FALSE}
pink_coffee_df = instacart %>% 
  select(order_hour_of_day, order_dow, product_name) %>% #select the data we are interested in
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% #filter out these two products
  group_by(order_dow, product_name) %>% 
  arrange(order_dow, order_hour_of_day) %>% 
  mutate(
    mean_order_hour = round(mean(order_hour_of_day) , digits = 0)
  )

pink_coffee_table = tibble(
  mean_order_hour = c(pull(pink_coffee_df, mean_order_hour)),
  order_day = c(pull(pink_coffee_df, order_dow)),
  product_name = c(pull(pink_coffee_df, product_name)) #create variables we want
) %>% 
  distinct() %>% #remove dupplicated rows
  mutate(
    order_day = recode(order_day, 
                       "0" = "Mon",
                       "1" = "Tue",
                       "2" = "Wed",
                       "3" = "Thu",
                       "4" = "Fri",
                       "5" = "Sat",
                       "6" = "Sun")
  ) %>% #transform integer to name of each day
  pivot_wider(
    names_from = product_name,
    values_from = mean_order_hour
  ) %>% #2x7 table
  janitor::clean_names() %>% 
  rename(pink_lady_apples_order_hr = 2) %>% 
  rename(coffee_ice_cream_order_hr = 3) #clean up names

```

# Section 1.4
Table 2: mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
```{r echo = FALSE}
knitr::kable(pink_coffee_table)
```

## Problem 2
```{r echo = FALSE}
#Import and clean data set
data("brfss_smart2010")

brfss2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr,  #rename variables
         county = locationdesc,
         health_class = class,
         response_id = respid) %>% 
  filter(topic == "Overall Health", 
         response != "Very good") %>% #filter the topic and filter out the response
  mutate(
    response = as.factor(response) #change type to factor
  ) %>% 
  arrange(desc(response)) #arrange from poor to excellent in descending order
```

# Section 2.1
```{r echo = FALSE}
locations2002_df = 
  brfss2010 %>% 
  filter(year == 2002) %>% 
  count(state) %>% 
  mutate(
    n_of_locations = n/4 #After observing dataset, I notice there are 4 responses for each location. Thus, the number of each distinct locations equals the total number of observations divded by 4
  ) %>% 
  filter(n_of_locations > 6) 

locations2010_df = 
  brfss2010 %>% 
  filter(year == 2010) %>% 
  count(state) %>% 
  mutate(
    n_of_locations = n/4 
  ) %>% 
  filter(n_of_locations > 6) 
```
Comments:

In 2002, `r c(pull(locations2002_df, state))` were observed at 7 or more locations. In 2010, `r c(pull(locations2010_df, state))` were observed at 7 or more locations.

# Section 2.2
```{r echo = FALSE}
# Construct dataset limited to "Excellent" response
excellent_resp = 
  brfss2010 %>% 
  group_by(year, state) %>% 
  select(year, state, response, data_value) %>% 
  filter(response == "Excellent")  %>% 
  drop_na() %>% #remove NA 
  mutate(
    mean_data_val = round(mean(data_value), digits = 2) #mean of data_value grouped by year and state
  ) %>% 
  select(-data_value,
         -response) %>% 
  distinct() #remove duplicated rows
```

```{r echo = FALSE}
excellent_resp %>% 
  ggplot(aes(x = year, y = mean_data_val, color = state)) +
  geom_line() +
  xlab("Year") +
  ylab("Average value") +
  ggtitle("Figure 2a: Average value within each state from 2002 to 2010")
```
 
```{r echo = FALSE}
excellent_resp %>% 
  ggplot(aes(x = year, y = mean_data_val, group = state)) +
  geom_point(size = 1) +
  geom_line() +
  facet_wrap(~state, nrow = 4) + 
  xlab("Year") +
  ylab("Average value") +
  ggtitle("Figure 2b: Figure 2a shown with separate plots based on states") +
  theme(axis.text.x = element_text(size = 5),
        axis.text.y = element_text(size = 8),
        axis.title = element_text(size = 15),
        strip.text = element_text(size = 10),
        title = element_text(size = 13),
        panel.spacing.x = unit(1, "lines")
  )
```

# Section 2.3 
Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r echo = FALSE}
NY_response = 
  brfss2010 %>% 
  select(year, state, county, data_value, response) %>% 
  filter(year == 2006 | year == 2010,
         state %in% c("NY"))

NY_response %>% 
  ggplot(aes(x = response, y = data_value, fill = county)) + 
  geom_col(position = "dodge") +
  facet_grid(~year) + 
  ylab("data value") +
  ggtitle("Figure 3: Distribution of data value for responses (“Poor” to “Excellent”) in NY") + 
  theme(title = element_text(size = 10))
  
```

